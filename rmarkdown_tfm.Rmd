---
title: "TFM UOC"
output: html_document
date: '2022-03-25'
---

## Hide warnings:

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

## Smarket

```{r}
# Imports:
library(caret)
library(foreach)
source("./codes/cohen_kappa_fun.R")
source("./codes/CI_MinimumSampleSize_fun.R")
source("./codes/fit_acc_fun.R")
```

```{r}
# ML algorithms:
source("./codes/knn_fun_parallel.R")
source("./codes/logistic_regression_fun_v2_parallel.R")
source("./codes/nb_fun_parallel.R")
source("./codes/rf_fun_parallel.R")
```


Data & param:

```{r}
# Data:
require(ISLR)
#attach(Smarket) # attach allows you to look up in R variables within a dataframe
# Predictor:
X_market <- Smarket[-ncol(Smarket)]
# Predict:
Y_market <- Smarket[ncol(Smarket)]
Y_market <- unlist(Y_market)
```


### kNN


```{r}
system.time(
minimum_sample_knn(X = X_market, 
                   Y = Y_market,
                   p_vec = 1:99/100,
                   thr_acc = 0.9,
                   n.cores = 1)
)
```




### Logistic regression

```{r}
system.time(
  Smarket_logreg <- minimum_sample_logistic_v2_parallel(
    X_market,
    Y_market,
    p_vec = 1:99/100, 
    thr_acc = 0.9, 
    n.cores = 1
    )
)
```

```{r}
fit_cohen <- nls(cohen_vec~(1-a)-b*training_set_size^c,
                      data = Smarket_logreg$df, 
                      start = list(a=0.5,b=0.5,c=-0.5),
                      control = nls.control(maxiter = 100, tol = 1e-8),
                      algorithm = "port"
  )
summary_fit_cohen <- summary(fit_cohen)
# Scatter 
plot(x = Smarket_logreg$df$training_set_size,
     y = Smarket_logreg$df$cohen_vec,
     xlab = "Training set size \n",
     ylab = "Cohen's kappa",
     sub = paste("Residual standard error:",
                 round(summary_fit_cohen$sigma,
                       digits = 5),
                 sep = " ")
     )
# Fit:
lines(Smarket_logreg$df$training_set_size,
        predict(fit_cohen,Smarket_logreg$df$training_set_size))
```


### Naive Bayes

```{r}
system.time(
minimum_sample_naivebayes(X = X_market, 
                          Y = Y_market,
                          p_vec = 1:99/100,
                          thr_acc = 0.9,
                          n.cores = 1)
)
```



### Random forest


```{r}
system.time(
minimum_sample_rf_parallel(X = X_market, 
                           Y = Y_market,
                           p_vec = 1:99/100,
                           thr_acc = 0.9,
                           n.cores = 8)
)
```

## iris

```{r}
# Imports:
library(caret)
library(foreach)
source("./codes/cohen_kappa_fun.R")
source("./codes/CI_MinimumSampleSize_fun.R")
source("./codes/fit_acc_fun.R")
```

```{r}
# ML algorithms:
source("./codes/knn_fun_parallel.R")
source("./codes/logistic_regression_fun_v2_parallel.R")
source("./codes/nb_fun_parallel.R")
source("./codes/rf_fun_parallel.R")
```


Data & param:

```{r}
# Data:
require(datasets)
#attach(Smarket) # attach allows you to look up in R variables within a dataframe
# Predictor:
X_iris <- iris[-ncol(iris)]
# Predict:
Y_iris <- iris[ncol(iris)]
Y_iris <- unlist(Y_iris)
```


### kNN


```{r}
system.time(
iris_knn <- minimum_sample_knn(
  X = X_iris, 
  Y = Y_iris,
  p_vec = 10:90/100,
  thr_acc = 0.9,
  n.cores = 1)
)
```

```{r}
fit_cohen <- nls(cohen_vec~(1-a)-b*training_set_size^c,
                      data = iris_knn$df, 
                      start = list(a=0.5,b=0.5,c=-0.5),
                      control = nls.control(maxiter = 100, tol = 1e-8),
                      algorithm = "port"
  )
summary_fit_cohen <- summary(fit_cohen)
# Scatter 
plot(x = iris_knn$df$training_set_size,
     y = iris_knn$df$cohen_vec,
     xlab = "Training set size \n",
     ylab = "Cohen's kappa",
     sub = paste("Residual standard error:",
                 round(summary_fit_cohen$sigma,
                       digits = 5),
                 sep = " ")
     )
# Fit:
lines(iris_knn$df$training_set_size,
        predict(fit_cohen,iris_knn$df$training_set_size))
```




### Random forest


```{r}
system.time(
iris_rf <-minimum_sample_rf_parallel(
  X = X_iris, 
  Y = Y_iris,
  p_vec = 10:90/100,
  thr_acc = 0.9,
  n.cores = 1)
)
```

```{r}
fit_cohen <- nls(cohen_vec~(1-a)-b*training_set_size^c,
                      data = iris_logreg$df, 
                      start = list(a=0.5,b=0.5,c=-0.5),
                      control = nls.control(maxiter = 100, tol = 1e-8),
                      algorithm = "port"
  )
summary_fit_cohen <- summary(fit_cohen)
# Scatter 
plot(x = Smarket_logreg$df$training_set_size,
     y = Smarket_logreg$df$cohen_vec,
     xlab = "Training set size \n",
     ylab = "Cohen's kappa",
     sub = paste("Residual standard error:",
                 round(summary_fit_cohen$sigma,
                       digits = 5),
                 sep = " ")
     )
# Fit:
lines(Smarket_logreg$df$training_set_size,
        predict(fit_cohen,Smarket_logreg$df$training_set_size))
```


## Winsconsin breast cancer dataset, normalized


```{r}
# Imports:
library(caret)
library(foreach)
source("./codes/cohen_kappa_fun.R")
source("./codes/CI_MinimumSampleSize_fun.R")
source("./codes/fit_acc_fun.R")
```

```{r}
# ML algorithms:
source("./codes/knn_fun_parallel.R")
source("./codes/logistic_regression_fun_v2_parallel.R")
source("./codes/nb_fun_parallel.R")
source("./codes/rf_fun_parallel.R")
```

```{r}
# Here we normalized the data of the
# Wisconsin breast cancer:
# wbcd <- read.csv("./data/classification/wisc_bc_data.csv",
#                  stringsAsFactors = FALSE)
# # id column provides no info, so we remove it:
# wbcd <- wbcd[-1]
# 
# # label diagnosis column:
# wbcd$diagnosis<- factor(wbcd$diagnosis,
#                         levels = c("B", "M"),
#                         labels = c("Benign", "Malignant")
#                         )
# 
# # Function for normalization:
# normalize <- function(x) {
# return ((x - min(x)) / (max(x) - min(x)))
# }
# 
# # create new df with normalized numerical columns:
# wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
# 
# # add diagnosis column to the end:
# wbcd_n["diagnosis"] <- wbcd[,1]

# # Save normalized df as csv:
# write.csv(x = wbcd_n,file = "./data/classification/wbcd_normalized.csv",quote = T,row.names = F,col.names = T)
```



```{r}
# Data:
wbcd_n <- read.csv("./data/classification/wbcd_normalized.csv",header = T,stringsAsFactors = T)
# head(wbcd_n)
# Variable to predict:
Y_wbcd_n <- wbcd_n[,ncol(wbcd_n)]
# Variables to use as predictors:
X_wbcd_n <- wbcd_n[,-ncol(wbcd_n)]
```

### kNN


```{r}
system.time(
minimum_sample_knn(X = X_wbcd_n, Y = Y_wbcd_n,
                  p_vec = 1:99/100,
                  thr_acc = 0.95,
                  n.cores = 1)
)
```




### Logistic regression

```{r}
system.time(
  minimum_sample_logistic_v2_parallel(X_wbcd_n,
                                      Y_wbcd_n,
                                      p_vec = 1:99/100, 
                                      thr_acc = 0.95, 
                                      n.cores = 1
                                      )
)
```



### Naive Bayes

```{r}
system.time(
minimum_sample_naivebayes(X = X_wbcd_n, Y = Y_wbcd_n,
                  p_vec = 1:99/100,
                  thr_acc = 0.95,
                  n.cores = 1)
)
```



### Random forest


```{r}
system.time(
minimum_sample_rf_parallel(X = X_wbcd_n, Y = Y_wbcd_n,
                  p_vec = 1:99/100,
                  thr_acc = 0.95,
                  n.cores = 8)
)
```





## Winsconsin breast cancer dataset, z-score standarization


```{r}
# Imports:
library(caret)
library(foreach)
source("./codes/cohen_kappa_fun.R")
source("./codes/CI_MinimumSampleSize_fun.R")
source("./codes/fit_acc_fun.R")
```



```{r}
# ML algorithms:
source("./codes/knn_fun_parallel.R")
source("./codes/logistic_regression_fun_v2_parallel.R")
source("./codes/nb_fun_parallel.R")
source("./codes/rf_fun_parallel.R")
```



```{r}
# Here we normalized the data of the
# Wisconsin breast cancer:
# wbcd <- read.csv("./data/classification/wisc_bc_data.csv",
#                  stringsAsFactors = FALSE)
# # id column provides no info, so we remove it:
# wbcd <- wbcd[-1]
# 
# # label diagnosis column:
# wbcd$diagnosis<- factor(wbcd$diagnosis,
#                         levels = c("B", "M"),
#                         labels = c("Benign", "Malignant")
#                         )
# 
# # create new df with normalized numerical columns:
# wbcd_z <- as.data.frame(scale(wbcd[2:31]))
# 
# # add diagnosis column to the end:
# wbcd_z["diagnosis"] <- wbcd[,1]
# 
# # # Save z-transformed df as csv:
# write.csv(x = wbcd_z,file = "./data/classification/wbcd_z.csv",quote = T,row.names = F,col.names = T)
```



```{r}
# Data:
wbcd_z <- read.csv("./data/classification/wbcd_normalized.csv",header = T,stringsAsFactors = T)
# Variable to predict:
Y_wbcd_z <- wbcd_z[,ncol(wbcd_z)]
# Variables to use as predictors:
X_wbcd_z <- wbcd_z[,-ncol(wbcd_z)]
```

### kNN


```{r}
system.time(
output_knn <- minimum_sample_knn(X = X_wbcd_z, Y = Y_wbcd_z,
                  p_vec = 1:99/100,
                  thr_acc = 0.9,
                  n.cores = 1)
)
```

```{r}
output_knn
```



### Logistic regression

```{r}
system.time(
output_logreg <- minimum_sample_logistic_v2_parallel(X_wbcd_z,
                                      Y_wbcd_z,
                                      p_vec = 1:99/100, 
                                      thr_acc = 0.9, 
                                      n.cores = 1
                                      )
)
```

```{r}
output_logreg
```



### Naive Bayes

```{r}
system.time(
output_nb <- minimum_sample_naivebayes(X = X_wbcd_z, 
                          Y = Y_wbcd_z,
                          p_vec = 1:99/100,
                          thr_acc = 0.9,
                          n.cores = 1)
)
```


```{r}
output_nb
```


### Random forest


```{r}
system.time(
output_rf <- minimum_sample_rf_parallel(X = X_wbcd_z, 
                                        Y = Y_wbcd_z,
                                        p_vec = 1:99/100,
                                        thr_acc = 0.9,
                                        n.cores = 8)
)
```


```{r}
output_rf
```




## Predicting age from DNA methylation (continuous data prediction)

```{r}
# Imports:
library(caret)
library(foreach)
source("./codes/cohen_kappa_fun.R")
source("./codes/CI_MinimumSampleSize_fun.R")
source("./codes/fit_acc_fun.R")
```



```{r}
# ML algorithms:
source("./codes/rf_fun_continuous.R")
```



```{r}
# file path for CpG methylation and age
fileMethAge=system.file("extdata",
                      "CpGmeth2Age.rds",
                      package="compGenomRData")

# read methylation-age table
ameth=readRDS(fileMethAge)

# The CpGs that have low variation are not likely to have any association with age; they could simply be technical variation of the experiment. We will remove CpGs that have less than 0.1 standard deviation.

ameth=ameth[,c(TRUE,matrixStats::colSds(as.matrix(ameth[,-1]))>0.1)]
```



```{r}
# Predictor:
X_ameth <- ameth[,-1]
# Predicted:
Y_ameth <- ameth[,1]
```


### Random forest


```{r}
system.time(
ameth_rf <- minimum_sample_rf_continuous(
  X = X_ameth, 
  Y = Y_ameth,
  p_vec = 1:9/10,
  thr_rmse = 10,
  n.cores = 8)
)
```













